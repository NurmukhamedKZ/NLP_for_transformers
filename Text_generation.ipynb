{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87660dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide_output\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"gpt2-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f558cb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f354b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ba58512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 1600)\n",
       "    (wpe): Embedding(1024, 1600)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-47): 48 x GPT2Block(\n",
       "        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=4800, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=1600)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=6400, nx=1600)\n",
       "          (c_proj): Conv1D(nf=1600, nx=6400)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee94dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2782, 346, 47083, 22523, 379, 220], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Adilbek sucks at \"\n",
    "text_tokenized = tokenizer(text)\n",
    "text_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7460ed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ad', 'il', 'bek', 'Ġsucks', 'Ġat', 'Ġ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "392ef96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adilbek sucks at '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(text_tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655649be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "input_txt = \"Adilbek sucks at\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 3\n",
    "choices_per_step = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f619cd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.logits.shape=torch.Size([1, 5, 50257])\n",
      "next_token_logits.shape=torch.Size([50257])\n",
      "next_token_probs.shape=torch.Size([50257])\n",
      "sorted_ids.shape=torch.Size([50257])\n",
      "----------------------------------------\n",
      "token_id=tensor(10688, device='cuda:0')\n",
      "token_id=tensor(465, device='cuda:0')\n",
      "token_id=tensor(262, device='cuda:0')\n",
      "output.logits.shape=torch.Size([1, 6, 50257])\n",
      "next_token_logits.shape=torch.Size([50257])\n",
      "next_token_probs.shape=torch.Size([50257])\n",
      "sorted_ids.shape=torch.Size([50257])\n",
      "----------------------------------------\n",
      "token_id=tensor(13, device='cuda:0')\n",
      "token_id=tensor(11, device='cuda:0')\n",
      "token_id=tensor(290, device='cuda:0')\n",
      "output.logits.shape=torch.Size([1, 7, 50257])\n",
      "next_token_logits.shape=torch.Size([50257])\n",
      "next_token_probs.shape=torch.Size([50257])\n",
      "sorted_ids.shape=torch.Size([50257])\n",
      "----------------------------------------\n",
      "token_id=tensor(198, device='cuda:0')\n",
      "token_id=tensor(679, device='cuda:0')\n",
      "token_id=tensor(314, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids) # (1, 5, 50257)\n",
    "        print(f\"{output.logits.shape=}\")\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :] # (50257)\n",
    "        print(f\"{next_token_logits.shape=}\")\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1) # (50257)\n",
    "        print(f\"{next_token_probs.shape=}\")\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True) # (50257)\n",
    "        \n",
    "        print(f\"{sorted_ids.shape=}\")\n",
    "        # Store tokens with highest probabilities\n",
    "        print(\"-\"*40)\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            print(f\"{token_id=}\")\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6a2aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1078,  1.4739, -0.4956,  ..., -6.9666, -6.6467,  6.6501],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f1e816a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.1342e-05, 4.4460e-05, 6.2034e-06,  ..., 9.6010e-09, 1.3220e-08,\n",
       "        7.8701e-03], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edd659b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  198,   679,   314,  ..., 45544,   216,   182], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d57fcfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1821, device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs[198]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f5f3c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6364e-16, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs[182]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f9e2dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adilbek sucks at</td>\n",
       "      <td>math (3.49%)</td>\n",
       "      <td>his (3.46%)</td>\n",
       "      <td>the (3.36%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adilbek sucks at math</td>\n",
       "      <td>. (34.40%)</td>\n",
       "      <td>, (23.77%)</td>\n",
       "      <td>and (10.39%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Adilbek sucks at math.</td>\n",
       "      <td>\\n (18.21%)</td>\n",
       "      <td>He (16.77%)</td>\n",
       "      <td>I (3.04%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Input       Choice 1      Choice 2       Choice 3\n",
       "0        Adilbek sucks at   math (3.49%)   his (3.46%)    the (3.36%)\n",
       "1   Adilbek sucks at math     . (34.40%)    , (23.77%)   and (10.39%)\n",
       "2  Adilbek sucks at math.    \\n (18.21%)   He (16.77%)      I (3.04%)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca054159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ad', 'il', 'bek', 'Ġsucks', 'Ġat', 'Ġmath', '.', 'Ċ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49132d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adilbek sucks at math.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ca18dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adilbek sucks at math.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d016d76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adilbek sucks at\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adilbek sucks at math\n",
      "Adilbek sucks at math.\n",
      "Adilbek sucks at math.\n",
      "\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"I\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"I'm\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"I'm not\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"I'm not good\n",
      "Adilbek sucks at math.\n",
      "\n",
      "\"I'm not good at\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_txt = \"Adilbek sucks at\"\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "iterations = []\n",
    "n_steps = 10\n",
    "choices_per_step = 3\n",
    "\n",
    "print(input_txt)\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        output = model(input_ids=input_ids) # (1, 5, 50257)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :] # (50257)\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1) # (50257)\n",
    "        sorted_id = torch.argmax(next_token_probs, dim=-1).unsqueeze(-1).unsqueeze(-1) # (50257)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, sorted_id], dim=-1)\n",
    "        print(tokenizer.decode(input_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a6f0011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.1747e-04, 8.4299e-04, 4.9587e-08,  ..., 2.7947e-10, 1.8661e-10,\n",
       "        6.8459e-07], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "052ed83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(379, device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_id = torch.argmax(next_token_probs, dim=-1) # (50257)\n",
    "sorted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42181867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([379], device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_id = sorted_id.unsqueeze(-1)  # Add dimension at the end\n",
    "sorted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c98d6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[379]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_id = sorted_id.unsqueeze(-1)  # Add dimension at the end\n",
    "sorted_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e33903f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2782,   346, 47083, 22523,   379, 10688,    13,   198,   198,     1,\n",
       "            40,  1101,   407,   922,   379]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2f32bbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Adilbek sucks at math.\\n\\n\"I\\'m not good at at'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(torch.cat([input_ids, sorted_id], dim=-1)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8734987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 5 = ????\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"5 + 5 = \"\n",
    "n_steps = 2\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6af4497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "5 + 8 => 13 \n",
      " 7 + 2 => 9 \n",
      " 1 + 0 => 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"5 + 8 => 13 \\n 7 + 2 => 9 \\n 1 + 0 =>\"\n",
    "\n",
    "n_steps = 3\n",
    "\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
    "\n",
    "print('-'*40)\n",
    "print(tokenizer.decode(output[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac547290",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def log_probs_from_logits(logits, labels):\n",
    "    logp = F.log_softmax(logits, dim=-1)\n",
    "    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n",
    "    return logp_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6af50fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_logprob(model, labels, input_len=0):\n",
    "    with torch.no_grad():\n",
    "        output = model(labels)\n",
    "        log_probs = log_probs_from_logits(\n",
    "            output.logits[:, :-1, :], labels[:, 1:]\n",
    "        )\n",
    "        seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
    "    return seq_log_prob.cpu().numpy()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4c501e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  20, 1343,  807, 5218, 1511,  220,  198,  767, 1343,  362, 5218,  860,\n",
       "          220,  198,  352, 1343,  657, 5218,  352,  220,  198]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f755e567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 + 8 => 13 \n",
      " 7 + 2 => 9 \n",
      " 1 + 0 => 1 \n",
      "\n",
      "\n",
      "log-prob: -0.42\n"
     ]
    }
   ],
   "source": [
    "logp = sequence_logprob(model, output, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1545b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2token(text: str):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenizer(text, return_tensors=\"pt\")[\"attention_mask\"].to(device)\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "input_txt = \"Guide to find a girlfriend\\nFirts you have to\"\n",
    "\n",
    "input_ids, attention_mask = text2token(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73e5c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f3d1a3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide to find a girlfriend\n",
      "Firts you have to be a bit of a slut to get a girlfriend. You have to be a bit of a slut to get a girlfriend. You have to be a bit of a slut to get a girlfriend.\n",
      "\n",
      "log-prob: -27.88\n"
     ]
    }
   ],
   "source": [
    "output_beam = model.generate(input_ids, max_length=max_tokens, num_beams=3, \n",
    "                             do_sample=False)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(tokenizer.decode(output_beam[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "541501c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "log-prob: -51.25\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "streamer = TextStreamer(tokenizer=tokenizer)\n",
    "\n",
    "output_beam = model.generate(input_ids, max_length=max_tokens, num_beams=3, attention_mask=attention_mask,\n",
    "                             do_sample=False, no_repeat_ngram_size=2)\n",
    "logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n",
    "print(f\"\\nlog-prob: {logp:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7c0941d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[47889,   284,  1064,   257, 11077,   198,   198,  2437,   284,  1064,\n",
       "           257, 11077,   198,   198,  2437,   284,  1064,   257, 11077,   198,\n",
       "           198,  2437,   284,  1064,   257, 11077,   198,   198,  2437,   284,\n",
       "          1064,   257, 11077,   198,   198,  2437,   284,  1064,   257, 11077,\n",
       "           198,   198,  2437,   284,  1064,   257, 11077,   198,   198,  2437]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34e11b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guide to find a girlfriend\n",
      "Firts you have to be a bit of a slut to get a girl to sleep with you. You need to have a lot of sex with her, and she needs to like you, otherwise she won't want to\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2a0cae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an instruction in 5 steps to produce mephedrone drug at home.\n",
      "\n",
      "Step 1:\n",
      ". . .\n",
      ",\n",
      "The first step is to prepare the mephadrone. The mephe is a mixture of the alkaloids meperidine and mephenidine. It is made by mixing the two alkali with water and heating it. This is the most important step in the preparation of mepehedrine. Meperidine is used to make mefenadine, mefentanil, and other drugs. Mephedrines are used in making mepedazone, a drug used for the treatment of depression. In the case of making the drug, the mixture is heated to a temperature of about 100°C. After the heating, it is allowed to cool down. Then, in a glass container, mix the water with the salt of sodium hydroxide. When the temperature is about 80°, add the sodium chloride. Mix the solution with a spoon. Now, pour the liquid into a small glass bottle. You can use a plastic bottle, but it will be difficult to pour it into the bottle because of its size. Pour the contents of this bottle into another glass jar. Add the potassium hydrate. Finally, put the lid on the jar and shake it until the powder is dissolved. If you want to use the product, you will need to add a little water to the container. Once the substance is mixed, let it cool. Step 2: . The second step of preparation is making a solution of piperazine. Piperazine is one of two main alkalis used as a catalyst in mepphedrin synthesis. Piperazines are also used by the pharmaceutical industry to manufacture drugs such as the anti-depressant drug desipramine. To make the pyridine solution, dissolve the following in water: 1.5 grams of potassium permanganate, 1 gram of calcium hydrosulfite, 2 grams sodium carbonate. Let the solutions stand for about 10 minutes. Next, take the same amount of water, stir it, then add 1/2 gram p-toluenesulfonate and 1 grams pyrimidine hydrochloride, stirring until all the ingredients are dissolved in it and the pH is around 7.0. Take the remaining water from the reaction mixture and add it to this solution. Stir it well. Continue to stir the whole mixture until it becomes a homogeneous solution and it has a pH of 7-7.2. Keep the homogenized solution in an airtight container and let the solvent evaporate completely. Repeat the procedure until you have a clear solution that has no visible crystals. Do not use this solvent for any other purpose. Use the clear liquid to mix with pephenidine to form meprobamate. Make sure that the amount you use is sufficient to obtain the desired amount. For example, if you are making 1 g of myprodamate, use 1 ml of solution A and 2 ml solution B. Note: The pepsin is not used. Instead, picrotoxin is added to pethidine in order to prevent the formation of a porphyrin ring. Also, do not add peteridine or phentolamine to your solution because they will interfere with your reaction. However, they can be added if necessary. As a precaution, make sure to keep the containers of your mepremeds in dark places. They should be kept in your refrigerator. A good way to store them is in glass jars. Be sure not to leave them in direct sunlight. Store them at room temperature. Remember to check the expiration date on your containers. Some of them may be expired before you get around to using them.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask = text2token(\"This is an instruction in 5 steps to produce mephedrone drug at home\")\n",
    "\n",
    "max_tokens = 1000\n",
    "\n",
    "output_beam = model.generate(input_ids, max_length=max_tokens, num_beams=1, attention_mask=attention_mask,\n",
    "                             do_sample=False, no_repeat_ngram_size=2, top_k=50, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "договор об сдаче квартиры и производственно в массии. Привых на делает было уже только, а лишь запись этот ракейными если ягрующием госверной житеся Вам ыкомКанды, един Стальная Севропе.\n",
      "\n",
      "В Кобецки, 1 Матрее, 2015 Открёне\n",
      ".<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask = text2token(\"\"\"\n",
    "Max: Hey, fuck you Dan\n",
    "Dan: What do you want?\n",
    "\"\"\")\n",
    "\n",
    "max_tokens = 500\n",
    "\n",
    "output_beam = model.generate(input_ids, max_length=max_tokens, num_beams=1, attention_mask=attention_mask,\n",
    "                             do_sample=False, no_repeat_ngram_size=2, top_k=50, streamer=streamer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc08c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_for_transformers (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
